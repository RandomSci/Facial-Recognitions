{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 1 laptop, 904.6ms\n",
      "Speed: 9.9ms preprocess, 904.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "person?  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torchvision import models, transforms as transforms, datasets\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, TensorDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from pytorch_lightning import LightningDataModule, LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from glob import glob\n",
    "from torch_snippets import * \n",
    "from ultralytics import YOLO \n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "#YOLO model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Train Images\n",
    "f = r\"/home/zkllmt/Documents/AI Section/Datasets/Facial_Recognition_Custom_Dataset/train/images/*\"\n",
    "ims = glob(f\"{f}/*\")\n",
    "\n",
    "#Classes\n",
    "cls_dir = r'/home/zkllmt/Documents/AI Section/Datasets/Facial_Recognition_Custom_Dataset/classes.txt'\n",
    "with open(cls_dir, 'r') as c:\n",
    "    classes = c.readlines()\n",
    "classes = classes[0].split(' ')\n",
    "classes = classes[:-1]\n",
    "label2target = {l:t+1 for t, l in enumerate(classes)}\n",
    "label2target[\"UnIdentified person\"] = 0\n",
    "target2label = {t:l for l, t in label2target.items()}\n",
    "\n",
    "# Sample Image (Change later for prediction)\n",
    "img = read(ims[-1])\n",
    "#Resize Image sample\n",
    "img_resized = resize(img, (640, 640))\n",
    "#YOLO Prediction\n",
    "pred = model(img_resized)[0]\n",
    "#Bollean for person prediction in yolo\n",
    "is_person_detected = any(box.cls == 0 for box in pred.boxes)\n",
    "print(\"person? \",is_person_detected)\n",
    "\n",
    "person_bbs = pred.boxes.xyxy\n",
    "x = person_bbs.tolist()\n",
    "x = [list(map(int, i)) for i in x]\n",
    "#real_bbs_person = [x[1]]\n",
    "\n",
    "ims = []\n",
    "for k in range(len(x)):\n",
    "    ims.append(img_resized[x[k][1]:x[k][3],x[k][0]:x[k][2]])\n",
    "\n",
    "#Use for Image classification\n",
    "im = [j for i,j in enumerate(ims)]\n",
    "#show(img_resized, bbs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 1 laptop, 405.2ms\n",
      "Speed: 5.5ms preprocess, 405.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Person detected?  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | ResNet           | 23.5 M | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.040    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zkllmt/anaconda3/envs/ForAI_Kernel/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zkllmt/anaconda3/envs/ForAI_Kernel/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  20%|██        | 2/10 [00:22<01:28,  0.09it/s, v_num=13]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torchvision import models, transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "from torch_snippets import *\n",
    "from torchsummary import summary\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "f = r\"/home/zkllmt/Documents/AI Section/Datasets/Facial_Recognition_Custom_Dataset/train/images/*\"\n",
    "ims = glob(f\"{f}/*\")\n",
    "\n",
    "cls_dir = r'/home/zkllmt/Documents/AI Section/Datasets/Facial_Recognition_Custom_Dataset/classes.txt'\n",
    "with open(cls_dir, 'r') as c:\n",
    "    classes = c.readlines()\n",
    "classes = classes[0].split(' ')\n",
    "classes = classes[:-1]\n",
    "label2target = {l:t+1 for t, l in enumerate(classes)}\n",
    "label2target[\"UnIdentified person\"] = 0\n",
    "target2label = {t:l for l, t in label2target.items()}\n",
    "\n",
    "img = read(ims[-1])\n",
    "img_resized = resize(img, (640, 640))\n",
    "\n",
    "pred = model(img_resized)[0]\n",
    "\n",
    "is_person_detected = any(box.cls == 0 for box in pred.boxes)\n",
    "print(\"Person detected? \", is_person_detected)\n",
    "\n",
    "person_bbs = pred.boxes.xyxy\n",
    "x = person_bbs.tolist()\n",
    "x = [list(map(int, i)) for i in x]\n",
    "\n",
    "ims = []\n",
    "for k in range(len(x)):\n",
    "    ims.append(img_resized[x[k][1]:x[k][3], x[k][0]:x[k][2]])\n",
    "\n",
    "im = [j for i,j in enumerate(ims)]\n",
    "\n",
    "classes_path = r'/home/zkllmt/Documents/AI Section/Datasets/Facial_Recognition_Custom_Dataset/classes.txt'\n",
    "with open(classes_path, \"r\") as F:\n",
    "    classes = F.readlines()\n",
    "\n",
    "train_tr = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_tr = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "df = pd.read_csv(\"charmae_only.csv\")\n",
    "img_dir = r'/home/zkllmt/Documents/AI Section/Datasets/Facial_Recognition_Custom_Dataset/train/images/train'\n",
    "classes = [i[:-1] for i in classes]\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, dir, df, transform=None):\n",
    "        self.dir = dir\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "        self.label2target = {label: idx for idx, label in enumerate(df['LabelName'].unique())}\n",
    "        self.target2label = {idx: label for label, idx in self.label2target.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        lbl = self.df['LabelName'][ix]\n",
    "        target = self.label2target[lbl]\n",
    "\n",
    "        im = self.df['images'][ix]\n",
    "        img_path = f\"{self.dir}/{im}\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        img, lbl = tuple(zip(*batch))\n",
    "        img = torch.stack(img)\n",
    "        lbl = torch.tensor(lbl, dtype=torch.long)\n",
    "        return img, lbl\n",
    "\n",
    "class DataModel(LightningDataModule):\n",
    "    def __init__(self, df=df, img_dir=img_dir, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir  \n",
    "\n",
    "        self.train_tr = T.Compose([\n",
    "            T.Resize(224),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.test_tr = T.Compose([\n",
    "            T.Resize(224),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        train_df, test_df = train_test_split(self.df, test_size=0.2, random_state=99)\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=99)\n",
    "        self.train_df = train_df.reset_index(drop=True)\n",
    "        self.test_df = test_df.reset_index(drop=True)\n",
    "        self.val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_data = Data(self.img_dir, self.train_df, transform=self.train_tr)\n",
    "        return DataLoader(train_data, batch_size=self.batch_size, shuffle=True, collate_fn=train_data.collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_data = Data(self.img_dir, self.test_df, transform=self.test_tr)\n",
    "        return DataLoader(test_data, batch_size=self.batch_size, shuffle=False, collate_fn=test_data.collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_data = Data(self.img_dir, self.val_df, transform=self.test_tr)\n",
    "        return DataLoader(val_data, batch_size=self.batch_size, shuffle=False, collate_fn=val_data.collate_fn)\n",
    "\n",
    "class ModelLightningModule(LightningModule):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.model = models.resnet50(weights='DEFAULT')\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.model.fc = nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, lbls = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.loss_fn(logits, lbls.long())\n",
    "        acc = (logits.argmax(1) == lbls).float().mean()\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, lbls = batch\n",
    "        logits = self(imgs)\n",
    "        val_loss = self.loss_fn(logits, lbls.long())\n",
    "        acc = (logits.argmax(1) == lbls).float().mean()\n",
    "        self.log('val_loss', val_loss)\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, lbls = batch\n",
    "        logits = self(imgs)\n",
    "        val_loss = self.loss_fn(logits, lbls.long())\n",
    "        acc = (logits.argmax(1) == lbls).float().mean()\n",
    "        self.log('val_loss', val_loss)\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_val_loss = self.trainer.callback_metrics['val_loss']\n",
    "        avg_val_acc = self.trainer.callback_metrics['val_acc']\n",
    "        self.log('val_loss', avg_val_loss, prog_bar=True)\n",
    "        self.log('val_acc', avg_val_acc, prog_bar=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_module = DataModel(batch_size=32)\n",
    "    model = ModelLightningModule()\n",
    "    trainer = Trainer(\n",
    "        max_epochs=10,\n",
    "        callbacks=[TQDMProgressBar()],\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    trainer.fit(model, data_module)\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    print(trainer.callback_metrics)\n",
    "    trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import *\n",
    "import face_recognition\n",
    "\n",
    "def encode_face(img_path):\n",
    "    \"\"\"Encodes a face from an image file path.\"\"\"\n",
    "    img = face_recognition.load_image_file(img_path)\n",
    "    encodings = face_recognition.face_encodings(img)\n",
    "    if len(encodings) > 0:\n",
    "        return encodings[0]  \n",
    "    else:\n",
    "        raise ValueError(f\"No face detected in {img_path}\")\n",
    "\n",
    "def compare(face_encoding1, face_encoding2, tolerance=0.4):\n",
    "    \"\"\"Compares two face encodings with a specified tolerance.\"\"\"\n",
    "    return face_recognition.compare_faces([face_encoding1], face_encoding2, tolerance=tolerance)[0]\n",
    "  \n",
    "im1 = r\"/home/zkllmt/Documents/AI Section/Datasets/Facial_Recognition_Custom_Dataset/train/images/train/eK3MiIuz.jpeg\"\n",
    "im2 = r\"/home/zkllmt/Documents/AI Section/Datasets/Facial_Recognition_Custom_Dataset/train/images/train/yzT-J9oK.jpeg\"\n",
    "im11 = r\"/home/zkllmt/Documents/AI Section/Datasets/Facial_Recognition_Custom_Dataset/test/images/person2/Selwyn.jpg\"\n",
    "\n",
    "try:\n",
    "    imz = encode_face(im1)\n",
    "    imz2 = encode_face(im2)\n",
    "    imzz = encode_face(im11)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "ims = [im1, im2, im11]\n",
    "encodeds = [imz, imz2, imzz]\n",
    "names = [\"charmae\", \"charmae\", \"Selwyn\"]\n",
    "\n",
    "\n",
    "\n",
    "for ix, encoded_face in enumerate(encodeds):\n",
    "    if compare(encoded_face, encodeds[-1]):\n",
    "        show(resize(read(ims[ix]), (100, 100)), title=names[ix])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForAI_Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
